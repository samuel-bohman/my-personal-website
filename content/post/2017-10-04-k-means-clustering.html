---
title: The K-Means Algorithm
author: Samuel Bohman
date: "2017-10-04"
tags: 
  - unsupervised learning
  - k-means
slug: the-k-means-algorithm
output: blogdown::html_page
summary: "A brief tutorial on the k-means algorithm."
---



<p>K-means is one of the most popular clustering algorithms. The goal of the algorithm is to partition a data set into a desired number of non-overlapping clusters <span class="math inline">\(K\)</span>, so that the total <em>within-cluster variation</em> <span class="math inline">\(W(C_K)\)</span> is minimized. Formally, we want to solve the problem:</p>
<p><span class="math display">\[
\ \min_{C_1,...,C_K} \left \{ \sum_{k=1}^K W(C_K) \right \}
\]</span></p>
<p>This is in fact a very difficult problem to solve precisely, since there are almost <span class="math inline">\(K^n\)</span> ways to partition <span class="math inline">\(n\)</span> observations into <span class="math inline">\(K\)</span> clusters. Fortunately, the k-means algorithm can find a pretty good solution to this problem.</p>
<p>The function <code>kmeans()</code> performs k-means clustering in R. We will illustrate the <code>k-means</code> algorithm below with some simulated data and the number of clusters <code>k = 3</code>.</p>
<pre class="r"><code>set.seed(5)
x &lt;- matrix(rnorm(50 * 2), ncol = 2)
x[1:25, 1] &lt;- x[1:25, 1] + 3
x[1:25, 2] &lt;- x[1:25, 2] - 4
x &lt;- as.data.frame(x)
km.out &lt;- kmeans(x = x, centers = 3, nstart = 10)</code></pre>
<p>You can see the cluster assignments of each data point by looking in <code>km.out$cluster</code>. Below, we make a table of the counts for each cluster.</p>
<pre class="r"><code>table(km.out$cluster)  </code></pre>
<pre><code>## 
##  1  2  3 
## 11 14 25</code></pre>
<p>We can plot the data with each observation colored according to its cluster assignment at each step of the assignment process to see how the algorithm works.</p>
<p><img src="/post/2017-10-04-k-means-clustering_files/figure-html/unnamed-chunk-3-1.png" width="576" /></p>
<p>The figure shows the process of the k-means algorithm with <code>k = 3</code>. The plot in the upper left corner shows just the data. The next plot shows the first iteration of the k-means algorithm. You can see that it has chosen three centroids arbitrarily somewhere in the middle of the plot and appointed every point in the dataset to the closest centroid (green, chocolate, or purple). For each iteration the algorithm recalculates the centroids based on the (new) cluster assignments of the data points. After some iterations, the centroids will stabilize and stop moving with each iteration. From the first iteration to the second iteration, the centroids moved a lot. But after the second iteration, they moved less.</p>
<p>Some of the algorithmic properties of k-means are:<br />
- For <span class="math inline">\(d = 1\)</span> the problem is solvable in polynomial time<br />
- NP-hard if <span class="math inline">\(d &gt;= 2\)</span><br />
- Finds a local optimum; often converges quickly<br />
- Guaranteed to converge after at most <span class="math inline">\(K^N\)</span> iterations<br />
- Choice of initial points can have a large influence on the result</p>
<p>Cheers! ðŸ˜„</p>
