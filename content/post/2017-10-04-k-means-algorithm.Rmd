---
title: Introduction to K-Means Clustering
author: Samuel Bohman
date: "2017-10-04"
tags: 
  - k-means
slug: introduction-to-kmeans-clustering
output: blogdown::html_page
summary: "This blog post gives a brief introduction to the k-means clustering algorithm."
---

https://www.datascience.com/blog/k-means-clustering

K-means is one of the most popular clustering algorithms. The goal of the algorithm is to partition a data set into a desired number of non-overlapping clusters $K$, so that the total *within-cluster variation* $W(C_K)$ is minimized. Formally, we want to solve the problem: 

$$
\ \min_{C_1,...,C_K} \left \{ \sum_{k=1}^K W(C_K) \right \}
$$

This is in fact a very difficult problem to solve precisely, since there are almost $K^n$ ways to partition $n$ observations into $K$ clusters. Fortunately, the k-means algorithm can find a pretty good solution to this problem. 

The function `kmeans()` performs k-means clustering in R. We will illustrate the `k-means` algorithm below with some simulated data and the number of clusters `k = 3`. 

```{r}
set.seed(5)
x <- matrix(rnorm(50 * 2), ncol = 2)
x[1:25, 1] <- x[1:25, 1] + 3
x[1:25, 2] <- x[1:25, 2] - 4
x <- as.data.frame(x)
km.out <- kmeans(x = x, centers = 3, nstart = 10)
```

You can see the cluster assignments of each data point by looking in `km.out$cluster`. Below, we make a table of the counts for each cluster.

```{r}
table(km.out$cluster)  
```

We can plot the data with each observation colored according to its cluster assignment at each step of the assignment process to see how the algorithm works. 

```{r echo = FALSE, fig.height = 3, fig.width = 6}
library(scales)
source("my_dist.R")
source("k_means.R")
palette(c("SeaGreen4", "chocolate", "MediumPurple4"))
par(cex = 0.6, ann = F, mai = c(0, 0, 0, 0), mfrow = c(2, 3))
set.seed(2)
k_means(x, k = 3, iter.max = 20)
```

The figure shows the process of the k-means algorithm with `k = 3`. The plot in the upper left corner shows just the data. The next plot shows the first iteration of the k-means algorithm. You can see that it has chosen three centroids arbitrarily somewhere in the middle of the plot and appointed every point in the dataset to the closest centroid (green, chocolate, or purple). For each iteration the algorithm recalculates the centroids based on the (new) cluster assignments of the data points. After some iterations, the centroids will stabilize and stop moving with each iteration. From the first iteration to the second iteration, the centroids moved a lot. But after the second iteration, they moved less. 

Some of the algorithmic properties of k-means are:  
- For $d = 1$ the problem is solvable in polynomial time  
- NP-hard if $d >= 2$  
- Finds a local optimum; often converges quickly  
- Guaranteed to converge after at most $K^N$ iterations  
- Choice of initial points can have a large influence on the result  

Cheers! `r emo::ji("smile")`
