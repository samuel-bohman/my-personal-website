---
title: The K-Means Algorithm
author: Samuel Bohman
date: "2017-10-04"
tags: 
  - unsupervised learning
  - k-means
slug: the-k-means-algorithm
output: blogdown::html_page
summary: "A brief tutorial on the k-means algorithm."
---

## The K-Means Algorithm

K-means is one of the most popular clustering algorithms. The goal of the K-means algorithm is to partition a data set into a desired number of non-overlapping clusters $K$, so that the total *within-cluster variation* $W(C_K)$ is minimized. Formally, we want to solve the problem: 

$$
\ \min_{C_1,...,C_K} \left \{ \sum_{k=1}^K W(C_K) \right \}
$$

This is in fact a very difficult problem to solve precisely, since there are almost $K^n$ ways to partition $n$ observations into $K$ clusters. Fortunately, the K-means algorithm can find a pretty good solution to this problem. 

#### Example

```{r}
load("cluster.RData")
set.seed(5)

my_dist <- function(a, b, metric = "euclidean") {
  if (metric == "manhattan") {
    dist <- sum(abs(a - b))  # manhattan distance
  } else {
    dist <- sqrt(sum((a - b) ^ 2))  # Euclidean distance
  }
  return(dist)
}

k_means <- function(x, k, max.iter = 20) {
  random_index <- sample(1:k, nrow(x), replace = TRUE)
  data_with_cluster <- cbind(x, clusterID = random_index)
  iterations <- 1
  plot(data_with_cluster[, 1:2])
  while(TRUE) {
    centroids <- matrix(rep(0, times = k * ncol(x)), nrow = k, ncol = ncol(x))
    for(i in 1:k) {
      obs_of_cluster_i <- data_with_cluster$clusterID == i
      centroids[i, ] <- colMeans(data_with_cluster[obs_of_cluster_i, 1:2])
    }
    dist_from_centroids <- matrix(rep(0, nrow(x) * k), nrow = nrow(x), ncol = k)
    for(i in 1:nrow(x)) {
      for(j in 1:nrow(centroids)) {
        dist_from_centroids[i, j] <- my_dist(x[i, ], centroids[j, ])
      }
    }
    obs_new_clusterID <- apply(dist_from_centroids, 1, which.min)
    if(all(obs_new_clusterID == data_with_cluster$clusterID)) {
      km.clusters <- obs_new_clusterID
      centroid.matrix <- centroids
      break
    } else if (iterations > max.iter) {
      break
    } else {
      data_with_cluster$clusterID <- obs_new_clusterID
      iterations <- iterations + 1
    }
    plot(data_with_cluster[, 1:2], col = data_with_cluster$clusterID)
    points(centroids[, 1:2], pch = 20, cex = 2, col = 1:k)
  }
  return(list("clusters" = km.clusters, "centroids" = centroid.matrix))
}

km_clusters <- k_means(cluster.data, k = 3, max.iter = 15)
```


Algorithmic properties:
- NP-hard if $d >= 2$  
- Finding the best solution in polynomial time is infeasible  
- For $d = 1$ the problem is solvable in polynomial time  
- Finds a local optimum  
- Often converges quickly  
- Relative efficient  
- Guaranteed to converge after at most $K^N$ iterations  
- Choice of initial points can have a large influence on the result  

