---
title: An Introduction to Clustering
author: Samuel Bohman
date: "2017-10-04"
tags: 
  - unsupervised learning
  - clustering
slug: clustering
output: blogdown::html_page
summary: "An introduction to clustering concepts and algorithms."
---

## Clustering, what is it? 

Clustering or cluster analysis is an unsupervised machine learning technique used for creating subgroups in a dataset such that the objects within a group are similar to one another and different from the objects in other groups. Technically speaking, we want to minimize the *intra-cluster* distances of objects and maximize the *inter-cluster* distances. There are many ways to do this, and in this brief introduction you are going to learn some common algorithms. 

## The Clustering Task

The first and foremost question is: How do we measure the distance between objects? The second question is: What is a good clustering? In other words, how do we measure the quality of a clustering?

Depending on your application domain and question, some distance metrics applies while others do not, and it is your task to figure out which distance metric you need. 

Typically we need two data structures in order to carry out clustering: 

(1) a data matrix (table/data frame) of $n$ objects (rows) and $d$ attributes or dimensions (columns), and 
(2) a distance matrix of all pairwise distances of the objects in the data matrix. 

In fact, with certain algorithms it is possible to do clustering only based on a distance matrix, which we will see later. 

## Distance Metrics

Now, let us turn to some distance metrics that we can use to measure the similarity between groups. There are many distance metrics; here we will take a look at three: the Euclidean distance, the Manhattan distance, and the Minkowski distance.  

### Euclidean distance

The Euclidean or direct "bird-flight" distance is by far the most commonly used distance metric. It can be thought of as the straight line distance between two points. It is defined as follows: 

$$
\left( \sum_{j=1}^P \left(x_{aj} - x_{bj}\right)^2 \right)^{1/2}
$$

#### Example

Assume we have two vectors: $X = [2, 2, 3, -1]$ and $Y = [1, 4, 4, 0]$. The Euclidean distance between $X$ and $Y$ can then be calculated as follows: 

$$L_2(x, y) = \sqrt{(2-1)^2 + (2-4)^2 + (3-4)^2 + (-1-0)^2}$$

$$=\sqrt{1^2 + (-2)^2 + (-1)^2 + (-1)^2}$$

$$=\sqrt{1 + 4 + 1 + 1}$$

$$=\sqrt{7} \approx 2.65$$

In R: 

```{r}
x <- c(2, 2, 3, -1)
y <- c(1, 4, 4, 0)
m <- matrix(data = c(x, y), byrow = T, nrow = 2)
dist(m, method = "euclidean")
```

### Manhattan Distance

The Manhattan or "city-block" distance (or "Pac-Man" distance `r emo::ji("smile")`) is another common metric used for datasets with binary predictors. 

```{r, echo = FALSE}
blogdown::shortcode("figure", 
                    src = "https://upload.wikimedia.org/wikipedia/commons/0/08/Manhattan_distance.svg", 
                    alt = "Manhattan distance")
```

In the figure, the blue, red, and yellow lines show various ways of getting from the lower left corner to the upper right corner using the grid layout. The green straight line shows the Euclidean distance in comparison. 

The Manhattan metric is defined as follows: 

$$
L_1(x, y) = \sum_{i = 1}^d | x_i - y_i |
$$

#### Example 

Assume we have two vectors: $X = [0, 5, 5, 6, 6]$ and $Y = [0, 0, 2, 2, 6]$ which corresponds to the yellow line in the figure. Thus, the Manhattan distance between $X$ and $Y$ can then be calculated as follows: 

$$L_1(x, y) = |0-0| + |5-0| + |5-2| + |6-2| + |6-6| = 12$$
In R, we can do the following: 

```{r}
# The yellow line
x <- c(0, 5, 5, 6, 6)
y <- c(0, 0, 2, 2, 6)
z <- matrix(c(x, y), byrow = T, nrow = 2)
dist(z, method = "manhattan")
```

### Minkowski Distance

The Minkowski distance can be considered a generalization of both the Euclidean distance and the Manhattan distance. It is defined as follows: 

$$
L_p(x, y) = \left( \sum_{i = 1}^d | x_i - y_i |^p \right)^{1/p}
$$
Minkowski distance is typically used with $p$ being 1 or 2, which correspond to $L_1$ or the Manhattan distance and $L_2$ or the Euclidean distance, respectively. 

## Partitioning Algorithms

The first group of clustering algorithms we are going to look at is called partitioning algorithms. The basic concept is to construct a partition of a set of $n$ objects into a set of $k$ clusters, in which each object belongs to *exactly one* cluster. 

### K-Means Clustering

K-means is one of the most popular clustering algorithms. The goal of the K-means algorithm is to partition a data set into a desired number of non-overlapping clusters $K$, so that the total *within-cluster variation* $W(C_K)$ is minimized. Formally, we want to solve the problem: 

$$
\ \min_{C_1,...,C_K} \left \{ \sum_{k=1}^K W(C_K) \right \}
$$

This is in fact a very difficult problem to solve precisely, since there are almost $K^n$ ways to partition $n$ observations into $K$ clusters. Fortunately, the K-means algorithm can find a pretty good solution to this problem. 

#### Example

```{r}
load("cluster.RData")
set.seed(5)

my_dist <- function(a, b, metric = "euclidean") {
  if (metric == "manhattan") {
    dist <- sum(abs(a - b))  # manhattan distance
  } else {
    dist <- sqrt(sum((a - b) ^ 2))  # Euclidean distance
  }
  return(dist)
}

k_means <- function(x, k, max.iter = 20) {
  random_index <- sample(1:k, nrow(x), replace = TRUE)
  data_with_cluster <- cbind(x, clusterID = random_index)
  iterations <- 1
  plot(data_with_cluster[, 1:2])
  while(TRUE) {
    centroids <- matrix(rep(0, times = k * ncol(x)), nrow = k, ncol = ncol(x))
    for(i in 1:k) {
      obs_of_cluster_i <- data_with_cluster$clusterID == i
      centroids[i, ] <- colMeans(data_with_cluster[obs_of_cluster_i, 1:2])
    }
    dist_from_centroids <- matrix(rep(0, nrow(x) * k), nrow = nrow(x), ncol = k)
    for(i in 1:nrow(x)) {
      for(j in 1:nrow(centroids)) {
        dist_from_centroids[i, j] <- my_dist(x[i, ], centroids[j, ])
      }
    }
    obs_new_clusterID <- apply(dist_from_centroids, 1, which.min)
    if(all(obs_new_clusterID == data_with_cluster$clusterID)) {
      km.clusters <- obs_new_clusterID
      centroid.matrix <- centroids
      break
    } else if (iterations > max.iter) {
      break
    } else {
      data_with_cluster$clusterID <- obs_new_clusterID
      iterations <- iterations + 1
    }
    plot(data_with_cluster[, 1:2], col = data_with_cluster$clusterID)
    points(centroids[, 1:2], pch = 20, cex = 2, col = 1:k)
  }
  return(list("clusters" = km.clusters, "centroids" = centroid.matrix))
}

km_clusters <- k_means(cluster.data, k = 3, max.iter = 15)
```


Algorithmic properties:
- NP-hard if $d >= 2$  
- Finding the best solution in polynomial time is infeasible  
- For $d = 1$ the problem is solvable in polynomial time  
- Finds a local optimum  
- Often converges quickly  
- Relative efficient  
- Guaranteed to converge after at most $K^N$ iterations  
- Choice of initial points can have a large influence on the result  

## K-Means++

An alternative as a way of avoiding the sometimes poor clustering found the by the standard *k*-means algorithm. 

## K-Median

The *K*-median algorithm is the same as the *K*-means algorithm, except that for each dimension we identify the median (the Manhattan distance) instead of the mean (the Euclidean distance).  

Algorithmic properties:  
- Less sensitive (more robust) against outliers  

## K-Medoids

Similar to *K*-means but the centers are objects from the data set, for example images.   

K-means vs. K-medoids:  
- K-medoids is more flexible: any distance/similarity measure  
- K-medoids is robust w.r.t. outliers  
- K-medoids is more expensive  

## Clustering Large Applications (CLARA)

Draws multiple samples of the data set, applies PAM on each sample, and gives the best clustering as output.   
- Strength: dealing with large data sets  
- Weakness: efficiency depends on sample size  
    
## Hierarchical Clustering

Produces a set of nested clusters organized as a hierarchical tree or dendogram. No assumption about the number of clusters.  

Agglomerative  
- Start with points as individual clusters  
- At each step, merge the closest pair of clusters until only one cluster remains  
Divisive  
- Start with one all-inclusive cluster  
- At each step, split a cluster until each cluster contains a point  

Distance between two clusters:

Single-link distance: minimum distance (maximum similarity) between two objects in $C_i$ and $C_j$  
  - Can handle non-elliptical shapes  
  - Sensitive to noise and outliers  
Complete-link distance: maximum distance (minimum similarity) between two objects in $C_i$ and $C_j$  
  - More balanced clusters  
  - Less sensitive to noise and outliers  
  - Tends to break large clusters - all clusters tend to have similar diameter  
Average-link distance: average distance between two objects in $C_i$ and $C_j$  
  - Compromise between single and complete  
Ward's distance: 


$$
(X, Y) \in \mathbb{R}^n
$$
