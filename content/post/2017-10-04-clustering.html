---
title: An Introduction to Clustering
author: Samuel Bohman
date: "2017-10-04"
tags: 
  - unsupervised learning
  - clustering
slug: clustering
output: blogdown::html_page
summary: "An introduction to clustering concepts and algorithms."
---



<div id="clustering-what-is-it" class="section level2">
<h2>Clustering, what is it?</h2>
<p>Clustering or cluster analysis is an unsupervised machine learning technique used for creating subgroups in a dataset such that the objects within a group are similar to one another and different from the objects in other groups. Technically speaking, we want to minimize the <em>intra-cluster</em> distances of objects and maximize the <em>inter-cluster</em> distances. There are many ways to do this, and in this brief introduction you are going to learn some common algorithms.</p>
</div>
<div id="the-clustering-task" class="section level2">
<h2>The Clustering Task</h2>
<p>The first and foremost question is: How do we measure the distance between objects? The second question is: What is a good clustering? In other words, how do we measure the quality of a clustering?</p>
<p>Depending on your application domain and question, some distance metrics applies while others do not, and it is your task to figure out which distance metric you need.</p>
<p>Typically we need two data structures in order to carry out clustering:</p>
<ol style="list-style-type: decimal">
<li>a data matrix (table/data frame) of <span class="math inline">\(n\)</span> objects (rows) and <span class="math inline">\(d\)</span> attributes or dimensions (columns), and</li>
<li>a distance matrix of all pairwise distances of the objects in the data matrix.</li>
</ol>
<p>In fact, with certain algorithms it is possible to do clustering only based on a distance matrix, which we will see later.</p>
</div>
<div id="distance-metrics" class="section level2">
<h2>Distance Metrics</h2>
<p>Now, let us turn to some distance metrics that we can use to measure the similarity between groups. There are many distance metrics; here we will take a look at three: the Euclidean distance, the Manhattan distance, and the Minkowski distance.</p>
<div id="euclidean-distance" class="section level3">
<h3>Euclidean distance</h3>
<p>The Euclidean or direct ‚Äúbird-flight‚Äù distance is by far the most commonly used distance metric. It can be thought of as the straight line distance between two points. It is defined as follows:</p>
<p><span class="math display">\[
\left( \sum_{j=1}^P \left(x_{aj} - x_{bj}\right)^2 \right)^{1/2}
\]</span></p>
<div id="example" class="section level4">
<h4>Example</h4>
<p>Assume we have two vectors: <span class="math inline">\(X = [2, 2, 3, -1]\)</span> and <span class="math inline">\(Y = [1, 4, 4, 0]\)</span>. The Euclidean distance between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> can then be calculated as follows:</p>
<p><span class="math display">\[L_2(x, y) = \sqrt{(2-1)^2 + (2-4)^2 + (3-4)^2 + (-1-0)^2}\]</span></p>
<p><span class="math display">\[=\sqrt{1^2 + (-2)^2 + (-1)^2 + (-1)^2}\]</span></p>
<p><span class="math display">\[=\sqrt{1 + 4 + 1 + 1}\]</span></p>
<p><span class="math display">\[=\sqrt{7} \approx 2.65\]</span></p>
<p>In R:</p>
<pre class="r"><code>x &lt;- c(2, 2, 3, -1)
y &lt;- c(1, 4, 4, 0)
m &lt;- matrix(data = c(x, y), byrow = T, nrow = 2)
dist(m, method = &quot;euclidean&quot;)</code></pre>
<pre><code>##          1
## 2 2.645751</code></pre>
</div>
</div>
<div id="manhattan-distance" class="section level3">
<h3>Manhattan Distance</h3>
<p>The Manhattan or ‚Äúcity-block‚Äù distance (or ‚ÄúPac-Man‚Äù distance üòÑ) is another common metric used for datasets with binary predictors.</p>
{{% figure src="https://upload.wikimedia.org/wikipedia/commons/0/08/Manhattan_distance.svg" alt="Manhattan distance" %}}
<p>In the figure, the blue, red, and yellow lines show various ways of getting from the lower left corner to the upper right corner using the grid layout. The green straight line shows the Euclidean distance in comparison.</p>
<p>The Manhattan metric is defined as follows:</p>
<p><span class="math display">\[
L_1(x, y) = \sum_{i = 1}^d | x_i - y_i |
\]</span></p>
<div id="example-1" class="section level4">
<h4>Example</h4>
<p>Assume we have two vectors: <span class="math inline">\(X = [0, 5, 5, 6, 6]\)</span> and <span class="math inline">\(Y = [0, 0, 2, 2, 6]\)</span> which corresponds to the yellow line in the figure. Thus, the Manhattan distance between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> can then be calculated as follows:</p>
<p><span class="math display">\[L_1(x, y) = |0-0| + |5-0| + |5-2| + |6-2| + |6-6| = 12\]</span> In R, we can do the following:</p>
<pre class="r"><code># The yellow line
x &lt;- c(0, 5, 5, 6, 6)
y &lt;- c(0, 0, 2, 2, 6)
z &lt;- matrix(c(x, y), byrow = T, nrow = 2)
dist(z, method = &quot;manhattan&quot;)</code></pre>
<pre><code>##    1
## 2 12</code></pre>
</div>
</div>
<div id="minkowski-distance" class="section level3">
<h3>Minkowski Distance</h3>
<p>The Minkowski distance can be considered a generalization of both the Euclidean distance and the Manhattan distance. It is defined as follows:</p>
<p><span class="math display">\[
L_p(x, y) = \left( \sum_{i = 1}^d | x_i - y_i |^p \right)^{1/p}
\]</span> Minkowski distance is typically used with <span class="math inline">\(p\)</span> being 1 or 2, which correspond to <span class="math inline">\(L_1\)</span> or the Manhattan distance and <span class="math inline">\(L_2\)</span> or the Euclidean distance, respectively.</p>
</div>
</div>
<div id="partitioning-algorithms" class="section level2">
<h2>Partitioning Algorithms</h2>
<p>The first group of clustering algorithms we are going to look at is called partitioning algorithms. The basic concept is to construct a partition of a set of <span class="math inline">\(n\)</span> objects into a set of <span class="math inline">\(k\)</span> clusters, in which each object belongs to <em>exactly one</em> cluster.</p>
<div id="k-means-clustering" class="section level3">
<h3>K-Means Clustering</h3>
<p>K-means is one of the most popular clustering algorithms. The goal of the K-means algorithm is to partition a data set into a desired number of non-overlapping clusters <span class="math inline">\(K\)</span>, so that the total <em>within-cluster variation</em> <span class="math inline">\(W(C_K)\)</span> is minimized. Formally, we want to solve the problem:</p>
<p><span class="math display">\[
\ \min_{C_1,...,C_K} \left \{ \sum_{k=1}^K W(C_K) \right \}
\]</span></p>
<p>This is in fact a very difficult problem to solve precisely, since there are almost <span class="math inline">\(K^n\)</span> ways to partition <span class="math inline">\(n\)</span> observations into <span class="math inline">\(K\)</span> clusters. Fortunately, the K-means algorithm can find a pretty good solution to this problem.</p>
<div id="example-2" class="section level4">
<h4>Example</h4>
<pre class="r"><code>load(&quot;cluster.RData&quot;)
set.seed(5)

my_dist &lt;- function(a, b, metric = &quot;euclidean&quot;) {
  if (metric == &quot;manhattan&quot;) {
    dist &lt;- sum(abs(a - b))  # manhattan distance
  } else {
    dist &lt;- sqrt(sum((a - b) ^ 2))  # Euclidean distance
  }
  return(dist)
}

k_means &lt;- function(x, k, max.iter = 20) {
  random_index &lt;- sample(1:k, nrow(x), replace = TRUE)
  data_with_cluster &lt;- cbind(x, clusterID = random_index)
  iterations &lt;- 1
  plot(data_with_cluster[, 1:2])
  while(TRUE) {
    centroids &lt;- matrix(rep(0, times = k * ncol(x)), nrow = k, ncol = ncol(x))
    for(i in 1:k) {
      obs_of_cluster_i &lt;- data_with_cluster$clusterID == i
      centroids[i, ] &lt;- colMeans(data_with_cluster[obs_of_cluster_i, 1:2])
    }
    dist_from_centroids &lt;- matrix(rep(0, nrow(x) * k), nrow = nrow(x), ncol = k)
    for(i in 1:nrow(x)) {
      for(j in 1:nrow(centroids)) {
        dist_from_centroids[i, j] &lt;- my_dist(x[i, ], centroids[j, ])
      }
    }
    obs_new_clusterID &lt;- apply(dist_from_centroids, 1, which.min)
    if(all(obs_new_clusterID == data_with_cluster$clusterID)) {
      km.clusters &lt;- obs_new_clusterID
      centroid.matrix &lt;- centroids
      break
    } else if (iterations &gt; max.iter) {
      break
    } else {
      data_with_cluster$clusterID &lt;- obs_new_clusterID
      iterations &lt;- iterations + 1
    }
    plot(data_with_cluster[, 1:2], col = data_with_cluster$clusterID)
    points(centroids[, 1:2], pch = 20, cex = 2, col = 1:k)
  }
  return(list(&quot;clusters&quot; = km.clusters, &quot;centroids&quot; = centroid.matrix))
}

km_clusters &lt;- k_means(cluster.data, k = 3, max.iter = 15)</code></pre>
<p><img src="/post/2017-10-04-clustering_files/figure-html/unnamed-chunk-4-1.png" width="672" /><img src="/post/2017-10-04-clustering_files/figure-html/unnamed-chunk-4-2.png" width="672" /><img src="/post/2017-10-04-clustering_files/figure-html/unnamed-chunk-4-3.png" width="672" /><img src="/post/2017-10-04-clustering_files/figure-html/unnamed-chunk-4-4.png" width="672" /><img src="/post/2017-10-04-clustering_files/figure-html/unnamed-chunk-4-5.png" width="672" /><img src="/post/2017-10-04-clustering_files/figure-html/unnamed-chunk-4-6.png" width="672" /><img src="/post/2017-10-04-clustering_files/figure-html/unnamed-chunk-4-7.png" width="672" /><img src="/post/2017-10-04-clustering_files/figure-html/unnamed-chunk-4-8.png" width="672" /></p>
<p>Algorithmic properties: - NP-hard if <span class="math inline">\(d &gt;= 2\)</span><br />
- Finding the best solution in polynomial time is infeasible<br />
- For <span class="math inline">\(d = 1\)</span> the problem is solvable in polynomial time<br />
- Finds a local optimum<br />
- Often converges quickly<br />
- Relative efficient<br />
- Guaranteed to converge after at most <span class="math inline">\(K^N\)</span> iterations<br />
- Choice of initial points can have a large influence on the result</p>
</div>
</div>
</div>
<div id="k-means" class="section level2">
<h2>K-Means++</h2>
<p>An alternative as a way of avoiding the sometimes poor clustering found the by the standard <em>k</em>-means algorithm.</p>
</div>
<div id="k-median" class="section level2">
<h2>K-Median</h2>
<p>The <em>K</em>-median algorithm is the same as the <em>K</em>-means algorithm, except that for each dimension we identify the median (the Manhattan distance) instead of the mean (the Euclidean distance).</p>
<p>Algorithmic properties:<br />
- Less sensitive (more robust) against outliers</p>
</div>
<div id="k-medoids" class="section level2">
<h2>K-Medoids</h2>
<p>Similar to <em>K</em>-means but the centers are objects from the data set, for example images.</p>
<p>K-means vs.¬†K-medoids:<br />
- K-medoids is more flexible: any distance/similarity measure<br />
- K-medoids is robust w.r.t. outliers<br />
- K-medoids is more expensive</p>
</div>
<div id="clustering-large-applications-clara" class="section level2">
<h2>Clustering Large Applications (CLARA)</h2>
<p>Draws multiple samples of the data set, applies PAM on each sample, and gives the best clustering as output.<br />
- Strength: dealing with large data sets<br />
- Weakness: efficiency depends on sample size</p>
</div>
<div id="hierarchical-clustering" class="section level2">
<h2>Hierarchical Clustering</h2>
<p>Produces a set of nested clusters organized as a hierarchical tree or dendogram. No assumption about the number of clusters.</p>
<p>Agglomerative<br />
- Start with points as individual clusters<br />
- At each step, merge the closest pair of clusters until only one cluster remains<br />
Divisive<br />
- Start with one all-inclusive cluster<br />
- At each step, split a cluster until each cluster contains a point</p>
<p>Distance between two clusters:</p>
<p>Single-link distance: minimum distance (maximum similarity) between two objects in <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_j\)</span><br />
- Can handle non-elliptical shapes<br />
- Sensitive to noise and outliers<br />
Complete-link distance: maximum distance (minimum similarity) between two objects in <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_j\)</span><br />
- More balanced clusters<br />
- Less sensitive to noise and outliers<br />
- Tends to break large clusters - all clusters tend to have similar diameter<br />
Average-link distance: average distance between two objects in <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_j\)</span><br />
- Compromise between single and complete<br />
Ward‚Äôs distance:</p>
<p><span class="math display">\[
(X, Y) \in \mathbb{R}^n
\]</span></p>
</div>
